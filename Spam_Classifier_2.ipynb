{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = r'SpamData\\02_Training\\train-data.txt'\n",
    "TEST_DATA = r'SpamData\\02_Training\\test-data.txt'\n",
    "WORD_LIST_FILE = 'SpamData/01_Processing/word-by-id.csv'\n",
    "\n",
    "PROB_TOKEN_SPAM = r'SpamData\\03_Testing\\prob-spam.txt'\n",
    "PROB_TOKEN_HAM = r'SpamData\\03_Testing\\prob-ham.txt'\n",
    "PROB_TOKEN_ALL = r'SpamData\\03_Testing\\prob-all.txt'\n",
    "\n",
    "TEST_FEATURE_MATRIX = r'SpamData\\03_Testing\\test-features.txt'\n",
    "TEST_TARGET_MATRIX = r'SpamData\\03_Testing\\teast-target.txt'\n",
    "\n",
    "VOCAB_SIZE=2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  1,  1],\n",
       "       [ 0,  3,  1,  2],\n",
       "       [ 0,  4,  1,  1],\n",
       "       [ 0,  7,  1,  3],\n",
       "       [ 0, 11,  1,  1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.loadtxt(TRAINING_DATA,delimiter=' ',dtype='int')\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2, 1, 1],\n",
       "       [8, 3, 1, 4],\n",
       "       [8, 4, 1, 2],\n",
       "       [8, 5, 1, 1],\n",
       "       [8, 6, 1, 2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.loadtxt(TEST_DATA,delimiter=' ',dtype='int')\n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = pd.read_csv(WORD_LIST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>VOCAB_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WORD_ID VOCAB_WORDS\n",
       "0        0        http\n",
       "1        1         use\n",
       "2        2        list\n",
       "3        3       email\n",
       "4        4         get"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Full Matrix from the Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty DataFrame : example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.unique(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DOC_ID']+['WORD_ID']+list(range(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2502"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = np.unique(train_data)\n",
    "# cols = ['DOC_ID']+['WORD_ID']+list(range(VOCAB_SIZE))\n",
    "# full_train_matrix = pd.DataFrame(index=indexes,columns=cols)\n",
    "# full_train_matrix.fillna(value=0,inplace=True)\n",
    "# full_train_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Function to fill the empty Full Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_matrix(data,VOCAB_SIZE,doc_index=0,word_index=1,class_index=2,freq_index=3):\n",
    "    \n",
    "    cols = ['DOC_ID']+['CLASS']+list(range(VOCAB_SIZE))\n",
    "        \n",
    "    indexes = np.unique(data[:,0])\n",
    "\n",
    "    full_matrix = pd.DataFrame(index=indexes,columns=cols)\n",
    "    full_matrix.fillna(value=0,inplace=True)\n",
    "    \n",
    "    for word in data:\n",
    "        DOC_ID=word[doc_index]\n",
    "        WORD_ID=word[word_index]\n",
    "        CLASS=word[class_index]\n",
    "        OCCURENCE=word[freq_index]\n",
    "        \n",
    "        full_matrix.at[DOC_ID,'DOC_ID']=DOC_ID\n",
    "        full_matrix.at[DOC_ID,'CLASS']=CLASS\n",
    "        full_matrix.at[DOC_ID,WORD_ID]=OCCURENCE\n",
    "    full_matrix.set_index('DOC_ID',inplace=True)\n",
    "    return full_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_train_matrix=create_full_matrix(data=train_data,VOCAB_SIZE=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_spam = full_train_matrix[full_train_matrix.CLASS==1].shape[0]/full_train_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ham = full_train_matrix[full_train_matrix.CLASS!=1].shape[0]/full_train_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = full_train_matrix.loc[:,full_train_matrix.columns!='CLASS'].sum(axis=1)\n",
    "total_word_count = total_word.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_word = total_word[full_train_matrix.CLASS==1]\n",
    "spam_word_count= spam_word.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word = total_word[full_train_matrix.CLASS==0]\n",
    "ham_word_count= ham_word.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam and Ham tokens summed by word ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_features = full_train_matrix.loc[:,full_train_matrix.columns!='CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sumby_id = full_train_features[full_train_matrix.CLASS==1].sum(axis=0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_sumby_id = full_train_features[full_train_matrix.CLASS==0].sum(axis=0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P(Token|Spam) - Probablity that the Token occurs given that the eamil is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_token_spam = spam_sumby_id / (spam_word_count+VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_token_spam.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P(Token|Ham) - Probablity that the Token occurs given that the eamil is not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_token_ham = ham_sumby_id / (ham_word_count+VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_token_ham.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P(Token) - Probablity that the Token occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laplace Smoothing not required for all tokens since they are the most 2500 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sumby_id = full_train_features.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_token = total_sumby_id / (total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_token.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_test_matrix=create_full_matrix(data=test_data,VOCAB_SIZE=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = full_test_matrix.loc[:,full_test_matrix.columns!='CLASS']\n",
    "Y_test = full_test_matrix.loc[:,full_test_matrix.columns=='CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(PROB_TOKEN_ALL,prob_token)\n",
    "np.savetxt(PROB_TOKEN_SPAM,prob_token_spam)\n",
    "np.savetxt(PROB_TOKEN_HAM,prob_token_ham)\n",
    "np.savetxt(TEST_FEATURE_MATRIX,X_test)\n",
    "np.savetxt(TEST_TARGET_MATRIX,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test = np.array(Y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_spam_prob = (X_test[0].dot(prob_token_spam) * prob_spam) / (X_test[0].dot(prob_token))\n",
    "# email_ham_prob = (X_test[0].dot(prob_token_ham) * prob_ham) / (X_test[0].dot(prob_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target=[]\n",
    "# for i in range(X_test.shape[0]):\n",
    "#     email_spam_prob = (X_test[i].dot(prob_token_spam) * prob_spam) / (X_test[i].dot(prob_token))\n",
    "#     email_ham_prob = (X_test[i].dot(prob_token_ham) * prob_ham) / (X_test[i].dot(prob_token))\n",
    "#     if email_ham_prob>email_spam_prob:\n",
    "#         target.append(0)\n",
    "#     else:\n",
    "#         target.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = {'Y_test':Y_test.flatten(),'Predicted':target}\n",
    "# result_df=pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_csv('abc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_prob_spam_log = X_test.dot(np.log(prob_token_spam)-np.log(prob_token))+np.log(prob_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_prob_ham_log = X_test.dot(np.log(prob_token_ham)-np.log(prob_token))+np.log(prob_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.85949456,  2.06515163, 17.57530083, 18.05639752, 19.70818369])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_prob_spam_log[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-59.28971636, -10.83968691, -33.3506051 , -58.22637041,\n",
       "       -53.15302715])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_prob_ham_log[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ... False False False]\n"
     ]
    }
   ],
   "source": [
    "predictions=joint_prob_spam_log>joint_prob_ham_log\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(Y_test==predictions).sum()/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779582366589327\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
